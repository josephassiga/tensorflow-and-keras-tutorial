{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephassiga/tensorflow-and-keras-tutorial/blob/main/W3/Lab5/W3_Lab_1_nlp_sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjwVLV2j8yb4"
      },
      "source": [
        "# Processing words as a sequence: The sequence model approach\n",
        "\n",
        "NLP is about: using machine learning and large datasets to\n",
        "give computers the ability not to understand language, which is a more lofty goal, but to ingest a piece of language as input and return something useful, like predicting the following:\n",
        "\n",
        "*  What’s the topic of this text?: text classification.\n",
        "*  Does this text contain abuse?: content filtering.\n",
        "*  Does this text sound positive or negative?: sentiment analysis.\n",
        "*  What should be the next word in this incomplete sentence?: language modeling\n",
        "*  How would you say this in German?: translation.\n",
        "*  How would you summarize this article in one paragraph?” : summarization.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn_Y_9Ye8ltW"
      },
      "source": [
        "# Preparing text data\n",
        "\n",
        "Deep learning models, being differentiable functions, can only process numeric tensors: they can’t take raw text as input. Vectorizing text is the process of transforming text into numeric tensors :\n",
        "\n",
        "* First, you standardize the text to make it easier to process, such as by converting it to lowercase or removing punctuation.\n",
        "* You split the text into units (called tokens), such as characters, words, or groups of words. This is called tokenization.\n",
        "* You convert each such token into a numerical vector. This will usually involve\n",
        "first indexing all tokens present in the data.\n",
        "\n",
        "Let's review each of these steps:\n",
        "\n",
        "![From raw text to vectors](https://github.com/josephassiga/tensorflow-and-keras-tutorial/blob/main/W3/Lab5/images/text-vectorization.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Standardization:\n",
        "\n",
        "    One of the simplest and most widespread standardization schemes are:\n",
        "    * Convert to lowercase and remove punctuation characters.\n",
        "    * Convert special characters to a standars form : replacing “é” with “e,” “æ” with “ae,” and so on.\n",
        "\n",
        " # Text Spliting :\n",
        "\n",
        "    Once your text is standardized, you need to break it up into units to be vectorized(tokens), a step called tokenization. You could do this in three different ways:\n",
        "    * Word-level tokenization—Where tokens are space-separated (or punctuationseparated) substrings.\n",
        "    * N-gram tokenization—Where tokens are groups of N consecutive words.\n",
        "    * Character-level tokenization—Where each character is its own token\n",
        "  In general, you’ll always use either **word-level** or **N-gram** tokenization\n"
      ],
      "metadata": {
        "id": "lRWQyoKtAsjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1a_xztRmY2Ka"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the IMDB movie review data"
      ],
      "metadata": {
        "id": "Uju2SYiVJL78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -Rf aclImdb\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "! rm -r aclImdb/train/unsup/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS9nlHB3JRM5",
        "outputId": "f1feb141-0ed1-4466-98c6-4a455b834fbe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  8537k      0  0:00:09  0:00:09 --:--:--  9.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-vQhNJBKJRY",
        "outputId": "a2caae28-8774-4577-d58c-e25164878ba3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the validation set by setting apart 20% of the training text files in a new directory :\n",
        "\n",
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = os.path.join(\"aclImdb\")\n",
        "val_dir = os.path.join(base_dir, \"val\")\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "\n",
        "for category in ['neg', 'pos']:\n",
        "  dst_dir = os.path.join(val_dir, category)\n",
        "  src_dir = os.path.join(train_dir, category)\n",
        "  os.makedirs(dst_dir)\n",
        "  files = os.listdir(src_dir)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for filename in val_files:\n",
        "    file_src = os.path.join(src_dir, filename)\n",
        "    file_dst = os.path.join(dst_dir,filename)\n",
        "    shutil.move(file_src,file_dst)\n"
      ],
      "metadata": {
        "id": "kaB242KwKcGK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        " \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        " \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        " \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaDUS9VlO8RH",
        "outputId": "8e0d23af-cbd2-4bfb-d5c3-cdceb77a0cd5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing our dataset :"
      ],
      "metadata": {
        "id": "yZsbwBntecQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encode the output tokens as multi-hot binary vectors.\n",
        "text_vectorization = TextVectorization(\n",
        " max_tokens=20000,\n",
        " output_mode=\"multi_hot\",\n",
        ")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        " lambda x, y: (text_vectorization(x), y),\n",
        " num_parallel_calls=4)\n",
        "\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        " lambda x, y: (text_vectorization(x), y),\n",
        " num_parallel_calls=4)\n",
        "\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        " lambda x, y: (text_vectorization(x), y),\n",
        " num_parallel_calls=4)\n"
      ],
      "metadata": {
        "id": "666glqXJea10"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the model"
      ],
      "metadata": {
        "id": "V9i6P1m5VyVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def show_accuracy_graph(history):\n",
        "  loss = history.history[\"accuracy\"]\n",
        "  val_loss = history.history[\"val_accuracy\"]\n",
        "  epochs = range(1, len(loss) + 1)\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss, \"bo\", label=\"Training Accuracy\")\n",
        "  plt.plot(epochs, val_loss, \"b\", label=\"Validation Accuracy\")\n",
        "  plt.title(\"Training and validation Accuract\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "0-p_SnLlV1wD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing integer sequence datasets"
      ],
      "metadata": {
        "id": "2fgoHj_GW1hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        " max_tokens=max_tokens,\n",
        " output_mode=\"int\",\n",
        " output_sequence_length=max_length,\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)\n",
        "\n",
        "int_val_ds = val_ds.map(\n",
        " lambda x, y: (text_vectorization(x), y),\n",
        " num_parallel_calls=4)\n",
        "\n",
        "int_test_ds = test_ds.map(\n",
        " lambda x, y: (text_vectorization(x), y),\n",
        " num_parallel_calls=4)\n",
        "\n",
        "callbacks = [\n",
        " keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        " save_best_only=True)\n",
        "]\n",
        "model = get_model()\n",
        "history = model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
        " callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "show_accuracy_graph(history)"
      ],
      "metadata": {
        "id": "ajdPNzV9eGb6",
        "outputId": "76d328c7-4360-4da0-f515-759a6000b234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 64)                5128448   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5128513 (19.56 MB)\n",
            "Trainable params: 5128513 (19.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 11793s 19s/step - loss: 0.5590 - accuracy: 0.7157 - val_loss: 0.3623 - val_accuracy: 0.8642\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.8607 "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}